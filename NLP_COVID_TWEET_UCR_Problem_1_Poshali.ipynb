{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName('COVID').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import os, tempfile\n",
    "import csv\n",
    "import string\n",
    "import random\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "data = spark.read.csv('Corona_NLP_train.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|         Sentiment|       OriginalTweet|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "|                3799|               48751|              London|          16-03-2020|           Neutral|@MeNyrbie @Phil_G...|\n",
      "|                3800|               48752|                  UK|          16-03-2020|          Positive|advice Talk to yo...|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|          Positive|Coronavirus Austr...|\n",
      "|                3802|               48754|                null|          16-03-2020|          Positive|My food stock is ...|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|              null|                null|\n",
      "|           Stay calm|          stay safe.|                null|                null|              null|                null|\n",
      "|#COVID19france #C...|                null|                null|                null|              null|                null|\n",
      "|                3803|               48755|                null|          16-03-2020|Extremely Negative|Me, ready to go a...|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|              null|                null|\n",
      "|#CoronavirusFranc...|                null|                null|                null|              null|                null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|          Positive|As news of the re...|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|          Positive|\"Cashier at groce...|\n",
      "|                3806|               48758|             Austria|          16-03-2020|           Neutral|Was at the superm...|\n",
      "|#toiletpapercrisi...|                null|                null|                null|              null|                null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|          Positive|Due to COVID-19 o...|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|          Negative|For corona preven...|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|           Neutral|All month there h...|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Extremely Positive|Due to the Covid-...|\n",
      "|The wait time may...| particularly bee...|                null|                null|              null|                null|\n",
      "|We thank you for ...|                null|                null|                null|              null|                null|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the length of tweets\n",
    "data=data.withColumn('length', length(data['OriginalTweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|         Sentiment|       OriginalTweet|length|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+------+\n",
      "|                3799|               48751|              London|          16-03-2020|           Neutral|@MeNyrbie @Phil_G...|   111|\n",
      "|                3800|               48752|                  UK|          16-03-2020|          Positive|advice Talk to yo...|   237|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|          Positive|Coronavirus Austr...|   131|\n",
      "|                3802|               48754|                null|          16-03-2020|          Positive|My food stock is ...|    51|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|              null|                null|  null|\n",
      "|           Stay calm|          stay safe.|                null|                null|              null|                null|  null|\n",
      "|#COVID19france #C...|                null|                null|                null|              null|                null|  null|\n",
      "|                3803|               48755|                null|          16-03-2020|Extremely Negative|Me, ready to go a...|    60|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|              null|                null|  null|\n",
      "|#CoronavirusFranc...|                null|                null|                null|              null|                null|  null|\n",
      "|                3804|               48756|ÜT: 36.319708,-82...|          16-03-2020|          Positive|As news of the re...|   249|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|          Positive|\"Cashier at groce...|   184|\n",
      "|                3806|               48758|             Austria|          16-03-2020|           Neutral|Was at the superm...|    61|\n",
      "|#toiletpapercrisi...|                null|                null|                null|              null|                null|  null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|          Positive|Due to COVID-19 o...|   280|\n",
      "|                3808|               48760|    BHAVNAGAR,GUJRAT|          16-03-2020|          Negative|For corona preven...|   267|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|           Neutral|All month there h...|   276|\n",
      "|                3810|               48762|Pitt Meadows, BC,...|          16-03-2020|Extremely Positive|Due to the Covid-...|    79|\n",
      "|The wait time may...| particularly bee...|                null|                null|              null|                null|  null|\n",
      "|We thank you for ...|                null|                null|                null|              null|                null|  null|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68046"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counting the number of rows\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UserName',\n",
       " 'ScreenName',\n",
       " 'Location',\n",
       " 'TweetAt',\n",
       " 'Sentiment',\n",
       " 'OriginalTweet',\n",
       " 'length']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing the various columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName             4\n",
       "ScreenName       24072\n",
       "Location         34547\n",
       "TweetAt          26552\n",
       "Sentiment        26752\n",
       "OriginalTweet    26835\n",
       "length           26835\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check For Missing Values\n",
    "data.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Missing Values\n",
    "data = data.dropna(subset=('Sentiment'))\n",
    "data = data.dropna(subset=('OriginalTweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('Sentiment', 'OriginalTweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         Sentiment|       OriginalTweet|\n",
      "+------------------+--------------------+\n",
      "|           Neutral|@MeNyrbie @Phil_G...|\n",
      "|          Positive|advice Talk to yo...|\n",
      "|          Positive|Coronavirus Austr...|\n",
      "|          Positive|My food stock is ...|\n",
      "|Extremely Negative|Me, ready to go a...|\n",
      "|          Positive|As news of the re...|\n",
      "|          Positive|\"Cashier at groce...|\n",
      "|           Neutral|Was at the superm...|\n",
      "|          Positive|Due to COVID-19 o...|\n",
      "|          Negative|For corona preven...|\n",
      "|           Neutral|All month there h...|\n",
      "|Extremely Positive|Due to the Covid-...|\n",
      "|Extremely Positive|#horningsea is a ...|\n",
      "|          Positive|Me: I don't need ...|\n",
      "|          Positive|ADARA Releases CO...|\n",
      "|          Positive|Lines at the groc...|\n",
      "|           Neutral|????? ????? ?????...|\n",
      "|           Neutral|\"@eyeonthearctic ...|\n",
      "|Extremely Positive|Amazon Glitch Sty...|\n",
      "|          Positive|For those who are...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41211"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Total number of rows\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries for cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the tweet\n",
    "data = data.withColumn('OriginalTweet', F.regexp_replace('OriginalTweet', r'http\\S+', ''))\n",
    "data = data.withColumn('OriginalTweet', F.regexp_replace('OriginalTweet', '@\\w+', ''))\n",
    "data = data.withColumn('OriginalTweet', F.regexp_replace('OriginalTweet', '#', ''))\n",
    "data = data.withColumn('OriginalTweet', F.regexp_replace('OriginalTweet', 'RT', ''))\n",
    "data = data.withColumn('OriginalTweet', F.regexp_replace('OriginalTweet', ':', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         Sentiment|       OriginalTweet|\n",
      "+------------------+--------------------+\n",
      "|           Neutral|           and  and |\n",
      "|          Positive|advice Talk to yo...|\n",
      "|          Positive|Coronavirus Austr...|\n",
      "|          Positive|My food stock is ...|\n",
      "|Extremely Negative|Me, ready to go a...|\n",
      "|          Positive|As news of the re...|\n",
      "|          Positive|\"Cashier at groce...|\n",
      "|           Neutral|Was at the superm...|\n",
      "|          Positive|Due to COVID-19 o...|\n",
      "|          Negative|For corona preven...|\n",
      "|           Neutral|All month there h...|\n",
      "|Extremely Positive|Due to the Covid-...|\n",
      "|Extremely Positive|horningsea is a c...|\n",
      "|          Positive|Me I don't need t...|\n",
      "|          Positive|ADARA Releases CO...|\n",
      "|          Positive|Lines at the groc...|\n",
      "|           Neutral|????? ????? ?????...|\n",
      "|           Neutral|\" 16MAR20 Russia ...|\n",
      "|Extremely Positive|Amazon Glitch Sty...|\n",
      "|          Positive|For those who are...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the columns to lower case\n",
    "from pyspark.sql.functions import lower, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('Sentiment', lower(col('Sentiment')))\n",
    "data = data.withColumn('OriginalTweet', lower(col('OriginalTweet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         Sentiment|       OriginalTweet|\n",
      "+------------------+--------------------+\n",
      "|           neutral|           and  and |\n",
      "|          positive|advice talk to yo...|\n",
      "|          positive|coronavirus austr...|\n",
      "|          positive|my food stock is ...|\n",
      "|extremely negative|me, ready to go a...|\n",
      "|          positive|as news of the re...|\n",
      "|          positive|\"cashier at groce...|\n",
      "|           neutral|was at the superm...|\n",
      "|          positive|due to covid-19 o...|\n",
      "|          negative|for corona preven...|\n",
      "|           neutral|all month there h...|\n",
      "|extremely positive|due to the covid-...|\n",
      "|extremely positive|horningsea is a c...|\n",
      "|          positive|me i don't need t...|\n",
      "|          positive|adara releases co...|\n",
      "|          positive|lines at the groc...|\n",
      "|           neutral|????? ????? ?????...|\n",
      "|           neutral|\" 16mar20 russia ...|\n",
      "|extremely positive|amazon glitch sty...|\n",
      "|          positive|for those who are...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Features Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Transformation\n",
    "import pyspark.ml.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various stages for pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "\n",
    "tokenizer=Tokenizer(inputCol=\"OriginalTweet\", outputCol=\"token_tweet\")\n",
    "stopremove=StopWordsRemover(inputCol=\"token_tweet\", outputCol=\"stop_tokens\")\n",
    "count_vec=CountVectorizer(inputCol=\"stop_tokens\", outputCol=\"c_vec\")\n",
    "idf=IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "\n",
    "# Converting the labels/sentiment to numbers(Label Encoding)\n",
    "labelEncoder = StringIndexer(inputCol=\"Sentiment\", outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|         Sentiment|       OriginalTweet|\n",
      "+------------------+--------------------+\n",
      "|           neutral|           and  and |\n",
      "|          positive|advice talk to yo...|\n",
      "|          positive|coronavirus austr...|\n",
      "|          positive|my food stock is ...|\n",
      "|extremely negative|me, ready to go a...|\n",
      "|          positive|as news of the re...|\n",
      "|          positive|\"cashier at groce...|\n",
      "|           neutral|was at the superm...|\n",
      "|          positive|due to covid-19 o...|\n",
      "|          negative|for corona preven...|\n",
      "+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the list of columns into a single vector column features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "data_pipeline= Pipeline(stages=[labelEncoder, tokenizer, stopremove,count_vec, idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline_fit=data_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data_pipeline_fit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         Sentiment|       OriginalTweet|label|         token_tweet|         stop_tokens|               c_vec|              tf_idf|            features|\n",
      "+------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           neutral|           and  and |  2.0|[, , , , and, , and]|          [, , , , ]|   (67136,[0],[5.0])|(67136,[0],[6.380...|(67136,[0],[6.380...|\n",
      "|          positive|advice talk to yo...|  0.0|[advice, talk, to...|[advice, talk, ne...|(67136,[14,15,132...|(67136,[14,15,132...|(67136,[14,15,132...|\n",
      "|          positive|coronavirus austr...|  0.0|[coronavirus, aus...|[coronavirus, aus...|(67136,[1,6,15,68...|(67136,[1,6,15,68...|(67136,[1,6,15,68...|\n",
      "|          positive|my food stock is ...|  0.0|[my, food, stock,...|[food, stock, one...|(67136,[5,33,35,1...|(67136,[5,33,35,1...|(67136,[5,33,35,1...|\n",
      "|extremely negative|me, ready to go a...|  4.0|[me,, ready, to, ...|[me,, ready, go, ...|(67136,[4,13,22,4...|(67136,[4,13,22,4...|(67136,[4,13,22,4...|\n",
      "|          positive|as news of the re...|  0.0|[as, news, of, th...|[news, regions, ...|(67136,[0,6,8,25,...|(67136,[0,6,8,25,...|(67136,[0,6,8,25,...|\n",
      "|          positive|\"cashier at groce...|  0.0|[\"cashier, at, gr...|[\"cashier, grocer...|(67136,[3,7,20,59...|(67136,[3,7,20,59...|(67136,[3,7,20,59...|\n",
      "|           neutral|was at the superm...|  2.0|[was, at, the, su...|[supermarket, tod...|(67136,[4,41,52,4...|(67136,[4,41,52,4...|(67136,[4,41,52,4...|\n",
      "|          positive|due to covid-19 o...|  0.0|[due, to, covid-1...|[due, covid-19, r...|(67136,[0,6,7,14,...|(67136,[0,6,7,14,...|(67136,[0,6,7,14,...|\n",
      "|          negative|for corona preven...|  1.0|[for, corona, pre...|[corona, preventi...|(67136,[11,14,15,...|(67136,[11,14,15,...|(67136,[11,14,15,...|\n",
      "|           neutral|all month there h...|  2.0|[all, month, ther...|[month, crowding,...|(67136,[55,70,78,...|(67136,[55,70,78,...|(67136,[55,70,78,...|\n",
      "|extremely positive|due to the covid-...|  3.0|[due, to, the, co...|[due, covid-19, s...|(67136,[5,6,24,32...|(67136,[5,6,24,32...|(67136,[5,6,24,32...|\n",
      "|extremely positive|horningsea is a c...|  3.0|[horningsea, is, ...|[horningsea, cari...|(67136,[14,15,28,...|(67136,[14,15,28,...|(67136,[14,15,28,...|\n",
      "|          positive|me i don't need t...|  0.0|[me, i, don't, ne...|[need, stock, foo...|(67136,[1,17,35,3...|(67136,[1,17,35,3...|(67136,[1,17,35,3...|\n",
      "|          positive|adara releases co...|  0.0|[adara, releases,...|[adara, releases,...|(67136,[6,9,28,49...|(67136,[6,9,28,49...|(67136,[6,9,28,49...|\n",
      "|          positive|lines at the groc...|  0.0|[lines, at, the, ...|[lines, grocery, ...|(67136,[3,7,115,3...|(67136,[3,7,115,3...|(67136,[3,7,115,3...|\n",
      "|           neutral|????? ????? ?????...|  2.0|[?????, ?????, ??...|[?????, ?????, ??...|(67136,[79,633],[...|(67136,[79,633],[...|(67136,[79,633],[...|\n",
      "|           neutral|\" 16mar20 russia ...|  2.0|[\", 16mar20, russ...|[\", 16mar20, russ...|(67136,[6,9,127,3...|(67136,[6,9,127,3...|(67136,[6,9,127,3...|\n",
      "|extremely positive|amazon glitch sty...|  3.0|[amazon, glitch, ...|[amazon, glitch, ...|(67136,[3,322,399...|(67136,[3,322,399...|(67136,[3,322,399...|\n",
      "|          positive|for those who are...|  0.0|[for, those, who,...|[struggling,, ple...|(67136,[5,6,24,40...|(67136,[5,6,24,40...|(67136,[5,6,24,40...|\n",
      "+------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the data after the pipeline implementation \n",
    "data_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data_final.select(['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  2.0|(67136,[0],[6.380...|\n",
      "|  0.0|(67136,[14,15,132...|\n",
      "|  0.0|(67136,[1,6,15,68...|\n",
      "|  0.0|(67136,[5,33,35,1...|\n",
      "|  4.0|(67136,[4,13,22,4...|\n",
      "|  0.0|(67136,[0,6,8,25,...|\n",
      "|  0.0|(67136,[3,7,20,59...|\n",
      "|  2.0|(67136,[4,41,52,4...|\n",
      "|  0.0|(67136,[0,6,7,14,...|\n",
      "|  1.0|(67136,[11,14,15,...|\n",
      "|  2.0|(67136,[55,70,78,...|\n",
      "|  3.0|(67136,[5,6,24,32...|\n",
      "|  3.0|(67136,[14,15,28,...|\n",
      "|  0.0|(67136,[1,17,35,3...|\n",
      "|  0.0|(67136,[6,9,28,49...|\n",
      "|  0.0|(67136,[3,7,115,3...|\n",
      "|  2.0|(67136,[79,633],[...|\n",
      "|  2.0|(67136,[6,9,127,3...|\n",
      "|  3.0|(67136,[3,322,399...|\n",
      "|  0.0|(67136,[5,6,24,40...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "(training, testing)=data_final.randomSplit([0.7,0.3],seed=123 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 2 classification models\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "\n",
    "rf=RandomForestClassifier(numTrees=50)\n",
    "dtc=DecisionTreeClassifier(maxDepth=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Training DecisionTreeClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Decision Tree Classification Model\n",
    "sentiment_predictor_dtc=dtc.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,6,6...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,8,2...|[545.0,102.0,24.0...|[0.46861564918314...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,13,...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,13,...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,13,...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,13,...|[7784.0,6887.0,60...|[0.27963787900560...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using Decision Tree Classification Model\n",
    "test_results_dtc=sentiment_predictor_dtc.transform(testing)\n",
    "test_results_dtc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the DecisionTreeClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval_dtc=MulticlassClassificationEvaluator()\n",
    "acc_dtc=acc_eval_dtc.evaluate(test_results_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier model is:: 0.20345812658311346\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of the Decision Tree Classifier model is::\", acc_dtc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Training RandomForestClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit RandomForestClassifier Model\n",
    "sentiment_predictor_rf=rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_results_rf=sentiment_predictor_rf.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|       (67136,[],[])|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0],[1.276...|[13.8117830542577...|[0.27623566108515...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,6,6...|[13.7261323359824...|[0.27452264671964...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,8,1...|[13.8058729722514...|[0.27611745944502...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,8,1...|[13.7261323359824...|[0.27452264671964...|       0.0|\n",
      "|  0.0|(67136,[0,1,2,8,2...|[13.9963053368100...|[0.27992610673620...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the RandomForestClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:: 0.28174317071216853\n"
     ]
    }
   ],
   "source": [
    "acc_eval_rf=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acc_rf=acc_eval_rf.evaluate(test_results_rf)\n",
    "\n",
    "print (\"Accuracy of the model is::\", acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is:: 28.174317071216855 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of the model is::\", acc_rf*100 ,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
